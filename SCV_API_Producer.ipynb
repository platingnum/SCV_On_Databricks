{"cells":[{"cell_type":"code","source":["# Read API from Kafka, Event hub, Sockets (for testing) and files . One  need to read data from API and store in file else these structures before processing by spark sreaming\n# create the base directory to store csv files\ndbutils.fs.rm(\"/FileStore/users\",recurse=True)\ndbutils.fs.mkdirs(\"/FileStore/users\")\ndbutils.fs.mkdirs(\"/FileStore/users/inprogress\")  #  needed and used to fetch data streams\ndbutils.fs.mkdirs(\"/FileStore/users/query\")   # Needed for advanced implementations "],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["dbutils.fs.mkdirs(\"/FileStore/users/checkpoints\") \n\n# Will track processing and in case failed will able to start processes from that point onwards\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["import schedule\nimport time\nimport requests\nimport datetime\nimport pandas as pd\nfrom pyspark.sql.functions import lit\n \ndef job():\n  print(\"calling CSV load function\")\n  url = \"https://my.api.mockaroo.com/users_load.json?key=6af9c3e0\"\n  \n  df = spark.createDataFrame(pd.read_csv(url))\n  \n  ts = time.time()\n  st = datetime.datetime.fromtimestamp(ts).strftime('%Y_%m_%d_%H_%M')\n  df_with_batch = df.withColumn(\"batch\", lit(datetime.datetime.fromtimestamp(ts).strftime('%Y_%m_%d_%H_%M_%S')))\n  fileName = '/FileStore/users/inprogress/'+ st + '.tmp'\n  fileprefix = '/FileStore/users/inprogress/'\n  df_with_batch.coalesce(1).write.format(\"com.databricks.spark.csv\") \\\n    .option(\"header\", True) \\\n    .option(\"quote\", \"\") \\\n    .save(fileName)  #saved to the FileStore\n    \n  fileList =  dbutils.fs.ls(fileName)\n\n  csvFileLocation = ''\n  for fileInfo in fileList:   \n    if \".csv\" in fileInfo.path:\n      print(\"this file is csv file..\" )\n      print(fileInfo.path)\n      csvFileLocation = fileprefix + fileInfo.name\n      \n      dbutils.fs.cp(fileInfo.path,fileprefix)\n      dbutils.fs.rm(fileName,recurse=True)\n  \n  #if (len(csvFileLocation) >0):\n  #  processUserInfo(csvFileLocation)\n  #  dbutills.fs.mv(csvFileLocation, '/FileStore/users/completed/')\n      \nschedule.every(20).seconds.do(job)\n \n\nwhile True:\n    schedule.run_pending()\n    time.sleep(1)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["dbutils.fs.ls(\"/FileStore/users/inprogress/\")"],"metadata":{},"outputs":[],"execution_count":4}],"metadata":{"name":"SCV_streaming","notebookId":3329407362443443},"nbformat":4,"nbformat_minor":0}
